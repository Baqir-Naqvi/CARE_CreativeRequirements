{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "The Length of sequences are:  263\n",
      "Data:  [[ 1  2  8  6 14  9  1 11  2 47]\n",
      " [ 2  8  6 14  9  1 11  2 47 48]\n",
      " [ 8  6 14  9  1 11  2 47 48  2]\n",
      " [ 6 14  9  1 11  2 47 48  2  4]\n",
      " [14  9  1 11  2 47 48  2  4  6]\n",
      " [ 9  1 11  2 47 48  2  4  6 14]\n",
      " [ 1 11  2 47 48  2  4  6 14  9]\n",
      " [11  2 47 48  2  4  6 14  9 11]\n",
      " [ 2 47 48  2  4  6 14  9 11  2]\n",
      " [47 48  2  4  6 14  9 11  2 49]]\n",
      "Response:  [48  2  4  6 14  9 11  2 49  1]\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 10, 128)      9856        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 10, 128)     527488      ['embedding_1[0][0]',            \n",
      " eadAttention)                                                    'embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 128)         0           ['multi_head_attention_1[0][0]'] \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 128)          0           ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1000)         129000      ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 77)           77077       ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 743,421\n",
      "Trainable params: 743,421\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.3352\n",
      "Epoch 1: loss improved from inf to 4.33518, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 1s 45ms/step - loss: 4.3352\n",
      "Epoch 2/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 4.2438\n",
      "Epoch 2: loss improved from 4.33518 to 4.23576, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 64ms/step - loss: 4.2358\n",
      "Epoch 3/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.9610\n",
      "Epoch 3: loss improved from 4.23576 to 3.96104, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 3.9610\n",
      "Epoch 4/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 3.9040\n",
      "Epoch 4: loss improved from 3.96104 to 3.90005, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 3.9001\n",
      "Epoch 5/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.8866\n",
      "Epoch 5: loss improved from 3.90005 to 3.88659, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 3.8866\n",
      "Epoch 6/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 3.8696\n",
      "Epoch 6: loss improved from 3.88659 to 3.85626, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 66ms/step - loss: 3.8563\n",
      "Epoch 7/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.8841\n",
      "Epoch 7: loss did not improve from 3.85626\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 3.8841\n",
      "Epoch 8/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.8363\n",
      "Epoch 8: loss improved from 3.85626 to 3.83627, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 3.8363\n",
      "Epoch 9/70\n",
      "3/5 [=================>............] - ETA: 0s - loss: 3.8717\n",
      "Epoch 9: loss did not improve from 3.83627\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 3.8478\n",
      "Epoch 10/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.8049\n",
      "Epoch 10: loss improved from 3.83627 to 3.80492, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 3.8049\n",
      "Epoch 11/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.7536\n",
      "Epoch 11: loss improved from 3.80492 to 3.75358, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 3.7536\n",
      "Epoch 12/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 3.6866\n",
      "Epoch 12: loss improved from 3.75358 to 3.68623, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 3.6862\n",
      "Epoch 13/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.5886\n",
      "Epoch 13: loss improved from 3.68623 to 3.58860, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 3.5886\n",
      "Epoch 14/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.4953\n",
      "Epoch 14: loss improved from 3.58860 to 3.49530, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 3.4953\n",
      "Epoch 15/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 3.3701\n",
      "Epoch 15: loss improved from 3.49530 to 3.36825, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 3.3682\n",
      "Epoch 16/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.2566\n",
      "Epoch 16: loss improved from 3.36825 to 3.25659, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 3.2566\n",
      "Epoch 17/70\n",
      "3/5 [=================>............] - ETA: 0s - loss: 3.1544\n",
      "Epoch 17: loss improved from 3.25659 to 3.13773, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 3.1377\n",
      "Epoch 18/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.0332\n",
      "Epoch 18: loss improved from 3.13773 to 3.03320, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 3.0332\n",
      "Epoch 19/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 2.9481\n",
      "Epoch 19: loss improved from 3.03320 to 2.95888, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 2.9589\n",
      "Epoch 20/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.8696\n",
      "Epoch 20: loss improved from 2.95888 to 2.86964, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 2.8696\n",
      "Epoch 21/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.8044\n",
      "Epoch 21: loss improved from 2.86964 to 2.80440, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 2.8044\n",
      "Epoch 22/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 2.6736\n",
      "Epoch 22: loss improved from 2.80440 to 2.70480, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 2.7048\n",
      "Epoch 23/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 2.6831\n",
      "Epoch 23: loss improved from 2.70480 to 2.68440, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 2.6844\n",
      "Epoch 24/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.6085\n",
      "Epoch 24: loss improved from 2.68440 to 2.60846, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 2.6085\n",
      "Epoch 25/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.5672\n",
      "Epoch 25: loss improved from 2.60846 to 2.56719, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 2.5672\n",
      "Epoch 26/70\n",
      "3/5 [=================>............] - ETA: 0s - loss: 2.4957\n",
      "Epoch 26: loss improved from 2.56719 to 2.50231, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 2.5023\n",
      "Epoch 27/70\n",
      "3/5 [=================>............] - ETA: 0s - loss: 2.3961\n",
      "Epoch 27: loss improved from 2.50231 to 2.40986, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 2.4099\n",
      "Epoch 28/70\n",
      "3/5 [=================>............] - ETA: 0s - loss: 2.2871\n",
      "Epoch 28: loss improved from 2.40986 to 2.33442, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 2.3344\n",
      "Epoch 29/70\n",
      "3/5 [=================>............] - ETA: 0s - loss: 2.3257\n",
      "Epoch 29: loss improved from 2.33442 to 2.28612, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 2.2861\n",
      "Epoch 30/70\n",
      "3/5 [=================>............] - ETA: 0s - loss: 2.1237\n",
      "Epoch 30: loss improved from 2.28612 to 2.18126, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 2.1813\n",
      "Epoch 31/70\n",
      "3/5 [=================>............] - ETA: 0s - loss: 2.1489\n",
      "Epoch 31: loss did not improve from 2.18126\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 2.2058\n",
      "Epoch 32/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 2.1195\n",
      "Epoch 32: loss improved from 2.18126 to 2.11021, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 2.1102\n",
      "Epoch 33/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.9876\n",
      "Epoch 33: loss improved from 2.11021 to 1.98761, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 1.9876\n",
      "Epoch 34/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.9536\n",
      "Epoch 34: loss improved from 1.98761 to 1.95364, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 1.9536\n",
      "Epoch 35/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.8652\n",
      "Epoch 35: loss improved from 1.95364 to 1.86516, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 1.8652\n",
      "Epoch 36/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.8172\n",
      "Epoch 36: loss improved from 1.86516 to 1.81718, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 1.8172\n",
      "Epoch 37/70\n",
      "3/5 [=================>............] - ETA: 0s - loss: 1.7253\n",
      "Epoch 37: loss improved from 1.81718 to 1.72898, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 1.7290\n",
      "Epoch 38/70\n",
      "3/5 [=================>............] - ETA: 0s - loss: 1.6967\n",
      "Epoch 38: loss did not improve from 1.72898\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 1.8081\n",
      "Epoch 39/70\n",
      "3/5 [=================>............] - ETA: 0s - loss: 1.6849\n",
      "Epoch 39: loss improved from 1.72898 to 1.68664, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 1.6866\n",
      "Epoch 40/70\n",
      "3/5 [=================>............] - ETA: 0s - loss: 1.5938\n",
      "Epoch 40: loss improved from 1.68664 to 1.56614, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 1.5661\n",
      "Epoch 41/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.5705\n",
      "Epoch 41: loss did not improve from 1.56614\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 1.5718\n",
      "Epoch 42/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.5365\n",
      "Epoch 42: loss improved from 1.56614 to 1.54257, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 1.5426\n",
      "Epoch 43/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.4593\n",
      "Epoch 43: loss improved from 1.54257 to 1.45965, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 1.4596\n",
      "Epoch 44/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.4266\n",
      "Epoch 44: loss improved from 1.45965 to 1.41858, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 1.4186\n",
      "Epoch 45/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.3938\n",
      "Epoch 45: loss improved from 1.41858 to 1.38624, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 103ms/step - loss: 1.3862\n",
      "Epoch 46/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.2716\n",
      "Epoch 46: loss improved from 1.38624 to 1.28376, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 106ms/step - loss: 1.2838\n",
      "Epoch 47/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.2759\n",
      "Epoch 47: loss improved from 1.28376 to 1.26713, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 1s 121ms/step - loss: 1.2671\n",
      "Epoch 48/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.2525\n",
      "Epoch 48: loss improved from 1.26713 to 1.23282, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 1.2328\n",
      "Epoch 49/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.1297\n",
      "Epoch 49: loss improved from 1.23282 to 1.12260, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.1226\n",
      "Epoch 50/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.1465\n",
      "Epoch 50: loss did not improve from 1.12260\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 1.1375\n",
      "Epoch 51/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.0791\n",
      "Epoch 51: loss improved from 1.12260 to 1.10037, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.1004\n",
      "Epoch 52/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.1235\n",
      "Epoch 52: loss did not improve from 1.10037\n",
      "5/5 [==============================] - 1s 96ms/step - loss: 1.1245\n",
      "Epoch 53/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.0793\n",
      "Epoch 53: loss improved from 1.10037 to 1.07607, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 1.0761\n",
      "Epoch 54/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.0427\n",
      "Epoch 54: loss improved from 1.07607 to 1.04410, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 1.0441\n",
      "Epoch 55/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.9819\n",
      "Epoch 55: loss improved from 1.04410 to 0.98608, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.9861\n",
      "Epoch 56/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.9334\n",
      "Epoch 56: loss improved from 0.98608 to 0.93955, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.9396\n",
      "Epoch 57/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.9654\n",
      "Epoch 57: loss did not improve from 0.93955\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.9747\n",
      "Epoch 58/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.9654\n",
      "Epoch 58: loss did not improve from 0.93955\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.9622\n",
      "Epoch 59/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.9415\n",
      "Epoch 59: loss improved from 0.93955 to 0.93799, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.9380\n",
      "Epoch 60/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.9018\n",
      "Epoch 60: loss improved from 0.93799 to 0.90156, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.9016\n",
      "Epoch 61/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.9482\n",
      "Epoch 61: loss did not improve from 0.90156\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.9455\n",
      "Epoch 62/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.9400\n",
      "Epoch 62: loss did not improve from 0.90156\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.9353\n",
      "Epoch 63/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.8383\n",
      "Epoch 63: loss improved from 0.90156 to 0.84219, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.8422\n",
      "Epoch 64/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.8318\n",
      "Epoch 64: loss improved from 0.84219 to 0.83561, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.8356\n",
      "Epoch 65/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.7820\n",
      "Epoch 65: loss improved from 0.83561 to 0.78746, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 1s 114ms/step - loss: 0.7875\n",
      "Epoch 66/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.7315\n",
      "Epoch 66: loss improved from 0.78746 to 0.73856, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.7386\n",
      "Epoch 67/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.6807\n",
      "Epoch 67: loss improved from 0.73856 to 0.69220, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 103ms/step - loss: 0.6922\n",
      "Epoch 68/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.7641\n",
      "Epoch 68: loss did not improve from 0.69220\n",
      "5/5 [==============================] - 0s 75ms/step - loss: 0.7776\n",
      "Epoch 69/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.7139\n",
      "Epoch 69: loss did not improve from 0.69220\n",
      "5/5 [==============================] - 0s 70ms/step - loss: 0.7316\n",
      "Epoch 70/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.7360\n",
      "Epoch 70: loss did not improve from 0.69220\n",
      "5/5 [==============================] - 0s 67ms/step - loss: 0.7458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23793b67130>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, GlobalAveragePooling1D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file = open('Requirements.txt', 'r')\n",
    "\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "    data = ' '.join(lines)\n",
    "\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“', '').replace('”', '')\n",
    "\n",
    "data = data.split()\n",
    "data = ' '.join(data)\n",
    "\n",
    "#remove all special characters and numbers and punctuations\n",
    "data = ''.join([i for i in data if not i.isdigit()])\n",
    "data = ''.join([i for i in data if i.isalpha() or i == ' '])\n",
    "\n",
    "data = data.lower()\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "sequences = []\n",
    "\n",
    "for i in range(10, len(sequence_data)):\n",
    "    words = sequence_data[i-10:i+1]\n",
    "    sequences.append(words)\n",
    "\n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:10])\n",
    "    y.append(i[10])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"Data: \", X[:10])\n",
    "print(\"Response: \", y[:10])\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "input_layer = Input(shape=(10,))\n",
    "embedding = Embedding(vocab_size, 128)(input_layer)\n",
    "transformer_block = MultiHeadAttention(num_heads=8, key_dim=128)(embedding, embedding)\n",
    "x = GlobalAveragePooling1D()(transformer_block)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1000, activation=\"relu\")(x)\n",
    "output_layer = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"next_words_transformer.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
    "model.fit(X, y, epochs=70, batch_size=64, callbacks=[checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['think', 'and', 'build']\n",
      "Error occurred:  in user code:\n",
      "\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n",
      "        return self(x, training=False)\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 10), found shape=(None, 3)\n",
      "\n",
      "['think']\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 1).\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "use\n",
      "['think', 'rebuild']\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "use\n",
      "['rebuild', 'and', '']\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "the\n",
      "['think', 'rebuild', 'stra']\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "use\n",
      "['think', 'rebuild', 'use']\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023795D750D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "and\n",
      "Execution completed.....\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "model = load_model('next_words_transformer.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    sequence = np.array(sequence)\n",
    "    preds = np.argmax(model.predict(sequence))\n",
    "    predicted_word = \"\"\n",
    "\n",
    "    for key, value in tokenizer.word_index.items():\n",
    "        if value == preds:\n",
    "            predicted_word = key\n",
    "            break\n",
    "\n",
    "    print(predicted_word)\n",
    "    return predicted_word\n",
    "\n",
    "while(True):\n",
    "    text = input(\"Enter your line: \")\n",
    "\n",
    "    if text == \"0\":\n",
    "        print(\"Execution completed.....\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            text = text.split(\" \")\n",
    "            text = text[-3:]\n",
    "            print(text)\n",
    "\n",
    "            Predict_Next_Words(model, tokenizer, text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred: \", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
