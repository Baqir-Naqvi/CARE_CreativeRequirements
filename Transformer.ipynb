{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "The Length of sequences are:  313\n",
      "Data:  [[ 1  2  9  7 19 10  1 14  2 11]\n",
      " [ 2  9  7 19 10  1 14  2 11  2]\n",
      " [ 9  7 19 10  1 14  2 11  2 11]\n",
      " [ 7 19 10  1 14  2 11  2 11  4]\n",
      " [19 10  1 14  2 11  2 11  4  1]\n",
      " [10  1 14  2 11  2 11  4  1  2]\n",
      " [ 1 14  2 11  2 11  4  1  2  5]\n",
      " [14  2 11  2 11  4  1  2  5  7]\n",
      " [ 2 11  2 11  4  1  2  5  7 19]\n",
      " [11  2 11  4  1  2  5  7 19 10]]\n",
      "Response:  [ 2 11  4  1  2  5  7 19 10 14]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 10, 128)      8064        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 10, 128)     527488      ['embedding[0][0]',              \n",
      " dAttention)                                                      'embedding[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 128)         0           ['multi_head_attention[0][0]']   \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1000)         129000      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 63)           63063       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 727,615\n",
      "Trainable params: 727,615\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.1281\n",
      "Epoch 1: loss improved from inf to 4.12807, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 1s 47ms/step - loss: 4.1281\n",
      "Epoch 2/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.9592\n",
      "Epoch 2: loss improved from 4.12807 to 3.95919, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 3.9592\n",
      "Epoch 3/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 3.7077\n",
      "Epoch 3: loss improved from 3.95919 to 3.75211, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 3.7521\n",
      "Epoch 4/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.6995\n",
      "Epoch 4: loss improved from 3.75211 to 3.69953, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 3.6995\n",
      "Epoch 5/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.6670\n",
      "Epoch 5: loss improved from 3.69953 to 3.66698, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 3.6670\n",
      "Epoch 6/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.6204\n",
      "Epoch 6: loss improved from 3.66698 to 3.62039, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 3.6204\n",
      "Epoch 7/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.5841\n",
      "Epoch 7: loss improved from 3.62039 to 3.58413, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 3.5841\n",
      "Epoch 8/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.5411\n",
      "Epoch 8: loss improved from 3.58413 to 3.54111, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 3.5411\n",
      "Epoch 9/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.4717\n",
      "Epoch 9: loss improved from 3.54111 to 3.47172, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 3.4717\n",
      "Epoch 10/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.4059\n",
      "Epoch 10: loss improved from 3.47172 to 3.40585, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 3.4059\n",
      "Epoch 11/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.2792\n",
      "Epoch 11: loss improved from 3.40585 to 3.27918, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 3.2792\n",
      "Epoch 12/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.1832\n",
      "Epoch 12: loss improved from 3.27918 to 3.18320, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 3.1832\n",
      "Epoch 13/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.0936\n",
      "Epoch 13: loss improved from 3.18320 to 3.09362, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 3.0936\n",
      "Epoch 14/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.9851\n",
      "Epoch 14: loss improved from 3.09362 to 2.98511, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 2.9851\n",
      "Epoch 15/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.8944\n",
      "Epoch 15: loss improved from 2.98511 to 2.89440, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 2.8944\n",
      "Epoch 16/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.7467\n",
      "Epoch 16: loss improved from 2.89440 to 2.74668, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 2.7467\n",
      "Epoch 17/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.6329\n",
      "Epoch 17: loss improved from 2.74668 to 2.63294, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 2.6329\n",
      "Epoch 18/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.4908\n",
      "Epoch 18: loss improved from 2.63294 to 2.49081, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 2.4908\n",
      "Epoch 19/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.4146\n",
      "Epoch 19: loss improved from 2.49081 to 2.41456, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 2.4146\n",
      "Epoch 20/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.2718\n",
      "Epoch 20: loss improved from 2.41456 to 2.27179, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 2.2718\n",
      "Epoch 21/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.1438\n",
      "Epoch 21: loss improved from 2.27179 to 2.14380, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 2.1438\n",
      "Epoch 22/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 2.0353\n",
      "Epoch 22: loss improved from 2.14380 to 2.03534, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 2.0353\n",
      "Epoch 23/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.9072\n",
      "Epoch 23: loss improved from 2.03534 to 1.92703, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 1.9270\n",
      "Epoch 24/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.8796\n",
      "Epoch 24: loss improved from 1.92703 to 1.87959, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 1.8796\n",
      "Epoch 25/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.7648\n",
      "Epoch 25: loss improved from 1.87959 to 1.76483, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 1.7648\n",
      "Epoch 26/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.7479\n",
      "Epoch 26: loss improved from 1.76483 to 1.74786, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 1.7479\n",
      "Epoch 27/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.5971\n",
      "Epoch 27: loss improved from 1.74786 to 1.59707, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 1.5971\n",
      "Epoch 28/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.5615\n",
      "Epoch 28: loss improved from 1.59707 to 1.56153, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 1.5615\n",
      "Epoch 29/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.4844\n",
      "Epoch 29: loss improved from 1.56153 to 1.48440, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 1.4844\n",
      "Epoch 30/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.3549\n",
      "Epoch 30: loss improved from 1.48440 to 1.35488, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 1.3549\n",
      "Epoch 31/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.3316\n",
      "Epoch 31: loss improved from 1.35488 to 1.33157, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 1.3316\n",
      "Epoch 32/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.2753\n",
      "Epoch 32: loss improved from 1.33157 to 1.27531, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 1.2753\n",
      "Epoch 33/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 1.2525\n",
      "Epoch 33: loss improved from 1.27531 to 1.21460, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 62ms/step - loss: 1.2146\n",
      "Epoch 34/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.1464\n",
      "Epoch 34: loss improved from 1.21460 to 1.14639, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 1.1464\n",
      "Epoch 35/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.0947\n",
      "Epoch 35: loss improved from 1.14639 to 1.09468, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 1.0947\n",
      "Epoch 36/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.0749\n",
      "Epoch 36: loss improved from 1.09468 to 1.07493, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 1.0749\n",
      "Epoch 37/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.9880\n",
      "Epoch 37: loss improved from 1.07493 to 0.98801, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.9880\n",
      "Epoch 38/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.9958\n",
      "Epoch 38: loss did not improve from 0.98801\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.9936\n",
      "Epoch 39/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.9205\n",
      "Epoch 39: loss improved from 0.98801 to 0.95672, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 0.9567\n",
      "Epoch 40/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.9307\n",
      "Epoch 40: loss improved from 0.95672 to 0.93068, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.9307\n",
      "Epoch 41/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.9038\n",
      "Epoch 41: loss improved from 0.93068 to 0.90379, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.9038\n",
      "Epoch 42/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.8838\n",
      "Epoch 42: loss improved from 0.90379 to 0.88376, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.8838\n",
      "Epoch 43/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.7928\n",
      "Epoch 43: loss did not improve from 0.88376\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.9035\n",
      "Epoch 44/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.8743\n",
      "Epoch 44: loss improved from 0.88376 to 0.87428, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.8743\n",
      "Epoch 45/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.8843\n",
      "Epoch 45: loss did not improve from 0.87428\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.8843\n",
      "Epoch 46/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.7828\n",
      "Epoch 46: loss improved from 0.87428 to 0.78283, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.7828\n",
      "Epoch 47/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.7641\n",
      "Epoch 47: loss improved from 0.78283 to 0.76410, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 0.7641\n",
      "Epoch 48/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.6928\n",
      "Epoch 48: loss improved from 0.76410 to 0.69278, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.6928\n",
      "Epoch 49/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.6823\n",
      "Epoch 49: loss improved from 0.69278 to 0.68227, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.6823\n",
      "Epoch 50/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.6423\n",
      "Epoch 50: loss improved from 0.68227 to 0.64232, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.6423\n",
      "Epoch 51/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.6559\n",
      "Epoch 51: loss did not improve from 0.64232\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.6559\n",
      "Epoch 52/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.6281\n",
      "Epoch 52: loss improved from 0.64232 to 0.62814, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.6281\n",
      "Epoch 53/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.6410\n",
      "Epoch 53: loss did not improve from 0.62814\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.6413\n",
      "Epoch 54/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.6378\n",
      "Epoch 54: loss did not improve from 0.62814\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.6378\n",
      "Epoch 55/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.6005\n",
      "Epoch 55: loss did not improve from 0.62814\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.6400\n",
      "Epoch 56/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.5861\n",
      "Epoch 56: loss improved from 0.62814 to 0.58614, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.5861\n",
      "Epoch 57/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.5458\n",
      "Epoch 57: loss improved from 0.58614 to 0.54584, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.5458\n",
      "Epoch 58/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.5273\n",
      "Epoch 58: loss improved from 0.54584 to 0.52729, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.5273\n",
      "Epoch 59/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.5450\n",
      "Epoch 59: loss did not improve from 0.52729\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.5450\n",
      "Epoch 60/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4984\n",
      "Epoch 60: loss improved from 0.52729 to 0.49837, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.4984\n",
      "Epoch 61/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.5051\n",
      "Epoch 61: loss did not improve from 0.49837\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.5051\n",
      "Epoch 62/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4998\n",
      "Epoch 62: loss did not improve from 0.49837\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.4998\n",
      "Epoch 63/70\n",
      "4/5 [=======================>......] - ETA: 0s - loss: 0.5409\n",
      "Epoch 63: loss did not improve from 0.49837\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.5568\n",
      "Epoch 64/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.5081\n",
      "Epoch 64: loss did not improve from 0.49837\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.5081\n",
      "Epoch 65/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4882\n",
      "Epoch 65: loss improved from 0.49837 to 0.48822, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.4882\n",
      "Epoch 66/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4511\n",
      "Epoch 66: loss improved from 0.48822 to 0.45113, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4511\n",
      "Epoch 67/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4264\n",
      "Epoch 67: loss improved from 0.45113 to 0.42638, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.4264\n",
      "Epoch 68/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4468\n",
      "Epoch 68: loss did not improve from 0.42638\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.4468\n",
      "Epoch 69/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4339\n",
      "Epoch 69: loss did not improve from 0.42638\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 0.4339\n",
      "Epoch 70/70\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.4027\n",
      "Epoch 70: loss improved from 0.42638 to 0.40266, saving model to next_words_transformer.h5\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.4027\n",
      "['game']\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, 1).\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "programmed\n",
      "['think', 'and', 'build']\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "game\n",
      "['the', 'user', 'to']\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "offer\n",
      "['user', 'to', '']\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "offer\n",
      "['horse', 'and', '']\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "game\n",
      "['men', 'and', '']\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "game\n",
      "['can', 'play', '']\n",
      "Error occurred:  in user code:\n",
      "\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n",
      "        return self(x, training=False)\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 10), found shape=(None, 0)\n",
      "\n",
      "['user', 'can', 'play']\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "a\n",
      "['can', 'play', 'a']\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "app\n",
      "['user', 'can', 'build']\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "a\n",
      "['build', 'a', '']\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "game\n",
      "['any', 'error', 'then']\n",
      "Error occurred:  in user code:\n",
      "\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n",
      "        return self(x, training=False)\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"c:\\Users\\WB\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 10), found shape=(None, 0)\n",
      "\n",
      "['user', 'to', 'think']\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "correctly\n",
      "['user', 'to', 'think']\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "correctly\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20824\\3665140593.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter your line: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"0\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\WB\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1175\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             )\n\u001b[1;32m-> 1177\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\WB\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1217\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1219\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1220\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, GlobalAveragePooling1D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file = open('Requirements.txt', 'r')\n",
    "\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "    data = ' '.join(lines)\n",
    "\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“', '').replace('”', '')\n",
    "\n",
    "data = data.split()\n",
    "data = ' '.join(data)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "sequences = []\n",
    "\n",
    "for i in range(10, len(sequence_data)):\n",
    "    words = sequence_data[i-10:i+1]\n",
    "    sequences.append(words)\n",
    "\n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:10])\n",
    "    y.append(i[10])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"Data: \", X[:10])\n",
    "print(\"Response: \", y[:10])\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "input_layer = Input(shape=(10,))\n",
    "embedding = Embedding(vocab_size, 128)(input_layer)\n",
    "transformer_block = MultiHeadAttention(num_heads=8, key_dim=128)(embedding, embedding)\n",
    "x = GlobalAveragePooling1D()(transformer_block)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1000, activation=\"relu\")(x)\n",
    "output_layer = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"next_words_transformer.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
    "model.fit(X, y, epochs=70, batch_size=64, callbacks=[checkpoint])\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "model = load_model('next_words_transformer.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    sequence = np.array(sequence)\n",
    "    preds = np.argmax(model.predict(sequence))\n",
    "    predicted_word = \"\"\n",
    "\n",
    "    for key, value in tokenizer.word_index.items():\n",
    "        if value == preds:\n",
    "            predicted_word = key\n",
    "            break\n",
    "\n",
    "    print(predicted_word)\n",
    "    return predicted_word\n",
    "\n",
    "while(True):\n",
    "    text = input(\"Enter your line: \")\n",
    "\n",
    "    if text == \"0\":\n",
    "        print(\"Execution completed.....\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            text = text.split(\" \")\n",
    "            text = text[-3:]\n",
    "            print(text)\n",
    "\n",
    "            Predict_Next_Words(model, tokenizer, text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred: \", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
